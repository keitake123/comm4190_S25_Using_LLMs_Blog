{
 "cells": [
  {
   "cell_type": "raw",
   "id": "07610c1e-6ff2-4e0d-bee1-7be205d8c9d4",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Taylor Sorensen on AI alignment with pluralistic values  \"\n",
    "description: \"Pluralistic Alignment: A Roadmap, Recent Work, and Open Problems\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - AI Alignment\n",
    "  - Talk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ea279-984a-4ecb-888e-e461f43202b9",
   "metadata": {},
   "source": [
    "<img src=\"profile.png\" width=400/>\n",
    "\n",
    "https://tsor13.github.io/  //\n",
    "\n",
    "https://drive.google.com/file/d/1FxhHmSOLE5Xpy_709Vf5_tfRfMBFNNCm/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ff2f1-9611-447d-9263-dd9bc73a501e",
   "metadata": {},
   "source": [
    "\n",
    "Speaker: Taylor Sorensen\n",
    "Mode: Virtual on Zoom\n",
    "Date & Time: Monday, February 10 12:00-1:30 PM\n",
    "\n",
    "Title: Pluralistic Alignment: A Roadmap, Recent Work, and Open Problems\n",
    "\n",
    "Abstract: Much alignment work assumes that there is a single set of human preferences or values that AI systems should align to. Pluralistic alignment, on the other hand, is concerned with integrating diverse human values and perspectives into alignment algorithms and evaluations. In this talk, I will start by presenting our roadmap to pluralistic alignment, offering definitions and scaffolding for research in the area. I will present one proposed algorithmic framework to support pluralism through modular specialist systems, along with a dataset and system to improve computational value-modeling. I will conclude with future research directions and open questions in the area.\n",
    "\n",
    "Bio: Taylor Sorensen is a CS PhD student at the University of Washington researching alignment, NLP for social good, and all things involving human values + LLMs. He's especially proud of his work on a roadmap and framework for pluralistic alignment, computational modeling of diverse human values, and using LLMs to help people with disagreement communicate more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929da2d-7288-4e9c-be0e-2507f7fdebd1",
   "metadata": {},
   "source": [
    "Mr. Taylor Sorensen’s talk was on pluralistic alignment in AI systems. This term refers to incorporating a diverse range of human values and preferences rather than a uniform single set. He talked about the importance of pluralistic alignment “Traditional machine learning systems often treat the variation between individuals as noise; however, this variation actually contains valuable signals reflecting the latent variables of people's values and preferences. By acknowledging and modeling these variables, AI systems can achieve a more accurate and nuanced understanding of the world.\" Mr. Sorensen discussed three models of pluralism: Overton Pluralism (summarizes all reasonable perspectives for a given query), Steerable Pluralism (focuses on the ability of a system to align with specific values or perspectives), and Distributional Pluralism (measures whether a model is well-calibrated towards a population from a particular country or group). Furthermore, he talked about Plurasitic benchmarks such as Multi-objective benchmarks to compare and contrast using multiple benchmarks, Trade-off steerable benchmarks which combine the multi-objective setting and the steerable setting, and Jury pluralism which learns a separate model for each individual rater and then models a whole population of individuals when getting an answer.\n",
    "\n",
    "In addition, Mr. Sorensen offered a case study that examines how current language model alignment techniques can inadvertently reduce distributional pluralism. In the last part of his talk, he talked about recent works and open problems that need to be solved. He introduced us to the value of kaleidoscope work and modular pluralism (multi-LM setup with specialist community LMs to achieve different kinds of pluralism). He discussed open problems for each model and benchmark such as whether reinforcement learning from human feedback fully solves Overton pluralism, or are there gaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9b21b-3efe-4e30-9974-0e00b93183f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
