{
 "cells": [
  {
   "cell_type": "raw",
   "id": "07610c1e-6ff2-4e0d-bee1-7be205d8c9d4",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Taylor Sorensen on AI alignment with pluralistic values  \"\n",
    "description: \"Pluralistic Alignment: A Roadmap, Recent Work, and Open Problems\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - AI Alignment\n",
    "  - Talk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ea279-984a-4ecb-888e-e461f43202b9",
   "metadata": {},
   "source": [
    "<img src=\"profile.png\" width=400/>\n",
    "\n",
    "https://tsor13.github.io/  //\n",
    "\n",
    "https://drive.google.com/file/d/1FxhHmSOLE5Xpy_709Vf5_tfRfMBFNNCm/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ff2f1-9611-447d-9263-dd9bc73a501e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "On Monday, February 10, I had the privilege of attending a virtual talk by Taylor Sorensen, a PhD student at the University of Washington. His talk centered on the concept of pluralistic alignment in AI systems, which seeks to go beyond the traditional idea of aligning AI with a single set of human values. Instead, pluralistic alignment focuses on incorporating a wide range of human perspectives, values, and preferences into the design and optimization of AI models. This concept is especially relevant in today’s world, where AI systems are increasingly integrated into various facets of life, and the potential consequences of biases in these systems can have far-reaching effects.\n",
    "\n",
    "During the talk, Sorensen provided a roadmap for pluralistic alignment, presenting algorithmic frameworks, models of pluralism, and evaluation benchmarks that allow for a more dynamic and nuanced approach to AI development. This framework pushes us to think of AI not just as a tool designed to mimic one version of human behavior, but as something that can be crafted to adapt to the diversity of human values, ultimately leading to more inclusive and fair systems.\n",
    "\n",
    "What stood out to me was the challenge of designing AI systems that can simultaneously respect different perspectives and how we can train these models using methods like Reinforcement Learning from Human Feedback (RLHF). The talk not only gave me insight into the current state of pluralistic alignment but also posed key questions and challenges that remain in the field.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Speaker: Taylor Sorensen\n",
    "Mode: Virtual on Zoom\n",
    "Date & Time: Monday, February 10 12:00-1:30 PM\n",
    "\n",
    "Title: Pluralistic Alignment: A Roadmap, Recent Work, and Open Problems\n",
    "\n",
    "Abstract: Much alignment work assumes that there is a single set of human preferences or values that AI systems should align to. Pluralistic alignment, on the other hand, is concerned with integrating diverse human values and perspectives into alignment algorithms and evaluations. In this talk, I will start by presenting our roadmap to pluralistic alignment, offering definitions and scaffolding for research in the area. I will present one proposed algorithmic framework to support pluralism through modular specialist systems, along with a dataset and system to improve computational value-modeling. I will conclude with future research directions and open questions in the area.\n",
    "\n",
    "Bio: Taylor Sorensen is a CS PhD student at the University of Washington researching alignment, NLP for social good, and all things involving human values + LLMs. He's especially proud of his work on a roadmap and framework for pluralistic alignment, computational modeling of diverse human values, and using LLMs to help people with disagreement communicate more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929da2d-7288-4e9c-be0e-2507f7fdebd1",
   "metadata": {},
   "source": [
    "## My summary on the talk\n",
    "\n",
    "Mr. Taylor Sorensen’s talk was on pluralistic alignment in AI systems. This term refers to incorporating a diverse range of human values and preferences rather than a uniform single set. He talked about the importance of pluralistic alignment “Traditional machine learning systems often treat the variation between individuals as noise; however, this variation actually contains valuable signals reflecting the latent variables of people's values and preferences. By acknowledging and modeling these variables, AI systems can achieve a more accurate and nuanced understanding of the world.\" Mr. Sorensen discussed three models of pluralism: Overton Pluralism (summarizes all reasonable perspectives for a given query), Steerable Pluralism (focuses on the ability of a system to align with specific values or perspectives), and Distributional Pluralism (measures whether a model is well-calibrated towards a population from a particular country or group). Furthermore, he talked about Plurasitic benchmarks such as Multi-objective benchmarks to compare and contrast using multiple benchmarks, Trade-off steerable benchmarks which combine the multi-objective setting and the steerable setting, and Jury pluralism which learns a separate model for each individual rater and then models a whole population of individuals when getting an answer.\n",
    "\n",
    "In addition, Mr. Sorensen offered a case study that examines how current language model alignment techniques can inadvertently reduce distributional pluralism. In the last part of his talk, he talked about recent works and open problems that need to be solved. He introduced us to the value of kaleidoscope work and modular pluralism (multi-LM setup with specialist community LMs to achieve different kinds of pluralism). He discussed open problems for each model and benchmark such as whether reinforcement learning from human feedback fully solves Overton pluralism, or are there gaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4395be-e0c1-436b-bc78-2d5189124b79",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Implementing pluralistic alignment in AI systems was a deeply thought-provoking experience, and Sorensen’s talk provided a roadmap that felt both innovative and necessary for the future of AI. Here’s what I took away from the session:\n",
    "\n",
    "1. RLHF aligns models with human preferences: The core idea of pluralistic alignment is that AI models should be trained with a wide spectrum of human values, rather than adhering to a single set. By integrating human feedback, AI can adapt and reflect diverse perspectives, making the system more adaptable and better suited for real-world applications where multiple viewpoints must be considered.\n",
    "\n",
    "2. Custom datasets and benchmarks are essential: One of the key challenges in pluralistic alignment is creating custom datasets that reflect diverse human feedback. Sorensen highlighted how Label Studio can be used to gather this feedback, which serves as the foundation for training models. Additionally, the development of pluralistic benchmarks is crucial for assessing whether these systems are successfully capturing and respecting human diversity in their outputs.\n",
    "\n",
    "3. PPO as an optimization tool: Proximal Policy Optimization (PPO) was discussed as a powerful tool for training AI systems based on human feedback. It strikes a balance between exploration and exploitation, allowing models to efficiently adapt while maintaining stability. This was one of the more practical insights from the talk — how RLHF can be implemented in a way that both optimizes and preserves diversity in model performance.\n",
    "\n",
    "4. Modular pluralism opens new possibilities: One of the exciting directions for pluralistic alignment is the idea of modular pluralism. By using specialized language models for different perspectives and combining them in a multi-LM setup, we can create systems that are better equipped to handle different kinds of pluralism. This approach could potentially allow AI to model not just general human preferences but also tailor responses to specific cultural, social, and individual values.\n",
    "\n",
    "5. Challenges and open problems remain: Sorensen emphasized that, despite the progress made, there are still open questions in pluralistic alignment, especially in terms of whether RLHF fully addresses the challenge of Overton pluralism (capturing all reasonable perspectives on a query). The talk posed the idea that while current techniques are promising, there’s still a lot of work to be done to ensure that AI systems reflect the full complexity of human values.\n",
    "\n",
    "Overall, Sorensen’s talk made me realize the importance of dynamic AI development, where systems can evolve based on human interaction and remain flexible in the face of diverse and changing societal needs. \n",
    "\n",
    "I’m excited to continue following this area of research, as pluralistic alignment seems like an essential approach for building AI systems that are truly aligned with the complexities of the human experience. This talk has inspired me to explore how we can combine human feedback with advanced AI techniques to create more inclusive and fair AI systems in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc93c8f-4971-4c2e-9d3b-d67c5ae820f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
