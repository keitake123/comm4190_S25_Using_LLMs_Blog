{
 "cells": [
  {
   "cell_type": "raw",
   "id": "984f90fa-df71-4dc3-8669-5faa2424b6a2",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 1: Learning about embeddings Similarity\"\n",
    "description: \"Learning about embeddings Similarity\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/7/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bfc20-8869-4d28-9c34-b9c59cfab048",
   "metadata": {},
   "source": [
    "I learned about embeddings and how we can find similarities between words and images so I am writing this in my blog. This is part 1 and part 2 will be looking into Semantic Search and K-NN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdb686-b966-4853-b031-ccf83d7c0562",
   "metadata": {},
   "source": [
    "# Understanding Embeddings\n",
    "\n",
    "## What Are Embeddings?\n",
    "\n",
    "Embeddings are a way to represent words, sentences, or even entire documents as lists of numbers (vectors). These numbers capture important patterns and relationships between different words or concepts. The idea is that similar words will have similar number patterns, allowing a computer to understand their meanings in a way that‚Äôs useful for tasks like search, recommendation systems, and machine learning.\n",
    "\n",
    "### Why Do We Use Embeddings?  \n",
    "Computers don‚Äôt understand words the way humans do‚Äîthey work with numbers. If we simply assigned each word a unique number (like \"apple\" = 1, \"banana\" = 2), the computer wouldn‚Äôt know that \"apple\" and \"banana\" are more related to each other than \"apple\" and \"airplane.\" Embeddings solve this problem by placing words in a mathematical space where related words have similar vector values.\n",
    "\n",
    "For example, in a good word embedding system:  \n",
    "- The word **\"king\"** might have a vector close to **\"queen\"** but farther from **\"dog.\"**  \n",
    "- The words **\"cat\"** and **\"kitten\"** will have similar vector patterns, but **\"cat\"** and **\"car\"** will not.  \n",
    "\n",
    "This is useful because it allows AI models to make sense of language, find relationships between words, and improve tasks like translating languages, recommending movies, or even detecting spam emails.\n",
    "\n",
    "### Key Benefits:\n",
    "1. **Semantic Understanding**: Captures relationships between concepts\n",
    "2. **Numerical Representation**: Converts text into machine-readable format\n",
    "3. **Similarity Comparison**: Enables efficient search and matching\n",
    "4. **Dimensionality Reduction**: Compresses complex information\n",
    "\n",
    "### Real-World Applications:\n",
    "- Semantic search engines\n",
    "- Recommendation systems\n",
    "- Document classification\n",
    "- Language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64846b8f-5553-48a9-a927-5449d92b7576",
   "metadata": {},
   "source": [
    "# üéØ **Generating Embeddings**\n",
    "## üß† What Are Embeddings?\n",
    "Embeddings convert words or entire articles into **numeric representations (vectors)** so that a computer can analyze their meaning and compare them.\n",
    "\n",
    "## üåü Why Use Embeddings?\n",
    "- They help computers **understand** text similarities.\n",
    "- They allow **semantic search**, where similar articles can be grouped together.\n",
    "- They power recommendation systems and search engines.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Embedding Function**\n",
    "This function:\n",
    "1. **Takes** a text input\n",
    "2. **Trims** it to 8000 characters (API limit)\n",
    "3. **Calls** OpenAI‚Äôs embedding model\n",
    "4. **Returns** a numerical representation of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962d00b-da8a-45dc-91ff-73a2d1f40d38",
   "metadata": {},
   "source": [
    "## üîÆ **Visualizing Word Embeddings**\n",
    "Words exist in a **high-dimensional space (1536D)**. To **see** relationships, we need to reduce the dimensions to **2D** using **t-SNE**.\n",
    "\n",
    "## üé® **What This Function Does**\n",
    "- Takes **lists of words**\n",
    "- Gets their **embeddings**\n",
    "- Uses **t-SNE** to reduce them to 2D\n",
    "- Plots them using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0a249-04d3-459c-979f-22a2cc2b8050",
   "metadata": {},
   "source": [
    "## üîé **Comparing Word Similarities**\n",
    "Now that we can **generate embeddings**, let's compare words based on their similarity using **cosine similarity**.\n",
    "\n",
    "## üß© **How Does It Work?**\n",
    "- Computes a **dot product** between two normalized embeddings.\n",
    "- Returns a similarity score **from -1 (opposite) to 1 (identical).**\n",
    "- Example: ‚Äúhappy‚Äù should be closer to ‚Äújoyful‚Äù than ‚Äúsad.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25c5e2-6f52-412b-b701-ae302ddb18c1",
   "metadata": {},
   "source": [
    "### Category-Based Image Search\n",
    "Try finding similar images for different categories and analyze the results:\n",
    "\n",
    "1. Find similar images for vehicles (cars, trucks, airplanes)\n",
    "2. Find similar images for animals (cats, dogs, birds)\n",
    "3. Compare how well the model distinguishes between similar categories\n",
    "4. Look at the similarity scores - what patterns do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5448b-a2d9-4325-a6ea-03fe67bf1136",
   "metadata": {},
   "source": [
    "<img src=\"a.png\" width=900/>\n",
    "<img src=\"b.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4436a7e-3540-4c95-8c01-7ecdaff75493",
   "metadata": {},
   "source": [
    "## Image Embeddings: A Visual Perspective\n",
    "\n",
    "Just like how we converted words into numerical vectors, we can do the same with images! Let's explore how image embeddings work using the CIFAR-10 dataset. This dataset is a common dataset used to train and test image classification models.\n",
    "\n",
    "### About CIFAR-10\n",
    "CIFAR-10 consists of 60,000 32x32 RGB images in 10 classes:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "We'll use:\n",
    "1. ResNet18 to generate embeddings\n",
    "2. FAISS for efficient similarity search\n",
    "3. t-SNE for visualization\n",
    "\n",
    "### Aside:\n",
    "ResNet18 is a convolutional neural network architecture with 18 layers, pre-trained on ImageNet. We're using it as a feature extractor by removing the final classification layer and using the penultimate layer's output as our image embeddings. These embeddings capture high-level visual features that are useful for similarity comparison between images. By using a pre-trained model, we can leverage the knowledge it has learned from ImageNet to generate meaningful representations of our CIFAR-10 images, even though they are from a different domain.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search of high-dimensional vectors. We're using it here to quickly find the most similar images in our dataset by comparing their ResNet18 embeddings. FAISS is optimized for handling high-dimensional vectors by an indexing mechanism.\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is a method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It tries to preserve the pairwise similarities between data points in a lower-dimensional space, allowing us to interpret or understand higher-dimensional data more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184be1a-de10-48af-b177-1d7cf948c513",
   "metadata": {},
   "source": [
    "<img src=\"c.png\" width=900/>\n",
    "<img src=\"d.png\" width=900/>\n",
    "<img src=\"e.png\" width=900/>\n",
    "<img src=\"f.png\" width=900/>\n",
    "<img src=\"g.png\" width=900/>\n",
    "<img src=\"h.png\" width=900/>\n",
    "<img src=\"i.png\" width=900/>\n",
    "<img src=\"j.png\" width=900/>\n",
    "<img src=\"k.png\" width=900/>\n",
    "<img src=\"l.png\" width=900/>\n",
    "<img src=\"m.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc68e27-cd57-4a7c-99c3-b9fb71a94a6b",
   "metadata": {},
   "source": [
    "### Aside: Why does the query not return good results?\n",
    "\n",
    "The query returns good results for some classes but not others because the quality of the similarity search depends on the quality of the embeddings. In this assignment, we use embeddings from ResNet-18's penultimate layer. ResNet-18 is trained for ImageNet classification and may capture general visual features well, but it might miss the fine-grained details necessary for distinguishing certain classes in CIFAR-10. In other words, ResNet-18's features are not always optimized for measuring similarity.\n",
    "\n",
    "We chose ResNet-18 for this assignment because it is one of the smaller and simpler models that works decently well. However, there are more advanced models designed specifically to learn semantically rich representations. Models such as CLIP, SimCLR, or MoCo‚Äîwhich use contrastive learning objectives‚Äîtend to produce embeddings that better capture subtle differences between classes. Additionally, fine-tuning a model on your dataset can further improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469b44f-b599-4a3b-ae7e-a79ded16ff38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
