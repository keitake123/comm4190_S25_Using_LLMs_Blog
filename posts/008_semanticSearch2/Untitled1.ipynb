{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ff09b100-45d3-4488-9133-8dc3cdba1dbf",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 2: Learning about embeddings Similarity\"\n",
    "description: \"Implementing Semantic Search and K-NN for wikipedia similarity\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/7/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cc58b-9682-443f-81ac-bc580f9205f7",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of my CIS 3990: Introduction to Artificial Intelligence class, we implemented one of the simplest yet most powerful machine learning algorithms ‚Äî **k-Nearest Neighbors (KNN). KNN is widely used in classification tasks due to its intuitive approach and straightforward implementation. \n",
    "\n",
    "The beauty of KNN lies in its simplicity: the algorithm makes predictions based on the closest data points in the training set. This makes it both effective and easy to understand.\n",
    "\n",
    "In this blog post, I‚Äôll walk you through how I implemented a KNN classifier from scratch and how it works in practice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa661dd-81c8-4459-8a68-8f1f4cba6124",
   "metadata": {},
   "source": [
    "#  **Implementing KNN (k-Nearest Neighbors)**\n",
    "KNN is a **simple** but **powerful** algorithm that:\n",
    "1. Stores **training data**\n",
    "2. Finds the **K closest** points to a test sample\n",
    "3. Assigns the **most common** label among those neighbors\n",
    "\n",
    "---\n",
    "\n",
    "##  **Building a Simple KNN Class**\n",
    "This class:\n",
    "- Stores training data (`X_train`, `y_train`)\n",
    "- Uses **Euclidean distance** to find nearest neighbors\n",
    "- Returns the **most common** label among them\n",
    "*italicized text*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ef8b6-247f-4b45-990d-92ad8cd6fe0e",
   "metadata": {},
   "source": [
    " Part 1: KNN Classifier Playground Goal: Build your own KNN classifier and see decision boundaries evolve!\n",
    "\n",
    " Step 1: Implement Simple KNN Help Professor AI finish the KNN class!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20762c27-cce2-46b7-9360-c3a123f909a2",
   "metadata": {},
   "source": [
    "## KNN Algorithm\n",
    "\n",
    "### Initialize SimpleKNN Class\n",
    "1. Set the number of neighbors (k) with default value 3\n",
    "\n",
    "### Train the model (fit function)\n",
    "1. Store the training data points (X_train)\n",
    "2. Store the corresponding labels (y_train)\n",
    "\n",
    "### Make predictions (predict function)\n",
    "1. Initialize empty list for predictions\n",
    "2. For each test point:\n",
    "   a. Calculate Euclidean distance to all training points\n",
    "      - Subtract test point from all training points\n",
    "      - Square the differences\n",
    "      - Sum along axis 1\n",
    "      - Take square root of sum\n",
    "   b. Find indices of k nearest neighbors\n",
    "      - Use argpartition function to get k smallest distances\n",
    "   c. Get labels of k nearest neighbors\n",
    "   d. Perform majority voting\n",
    "      - Use Counter to count label occurrences\n",
    "      - Find most common label\n",
    "   e. Append most common label to predictions\n",
    "\n",
    "3. Return predictions as a numpy array\n",
    "\n",
    "## Helper Functions\n",
    "1. numpy.sqrt(): Calculate square root\n",
    "2. numpy.sum(): Sum array elements\n",
    "3. numpy.argpartition(): Partially sort array to find k smallest elements\n",
    "4. Counter(): Count occurrences of elements in a list\n",
    "5. most_common(): Get most common element(s) from Counter object/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141150b-87bc-4273-8ca5-c3bb7b3a3b4b",
   "metadata": {},
   "source": [
    "<img src=\"a.png\" width=900/>\n",
    "\n",
    "## üè¶ MNIST Data & KNN Boundaries\n",
    "We load the **digits** dataset, scale, and use **PCA** to reduce the features to 2 dimensions.\n",
    "- This helps us **plot** decision boundaries for each digit (0-9).\n",
    "- We then **draw** lines/regions indicating which digit the KNN would classify a point as.\n",
    "\n",
    "\n",
    "üìä Visualizing KNN on MNIST Digits Goal: Explore how KNN performs on PCA-reduced MNIST digits!\n",
    "\n",
    "(Example implementation based on your KNN implementation)\n",
    "\n",
    "<img src=\"b.png\" width=900/>\n",
    "<img src=\"c.png\" width=900/>\n",
    "<img src=\"d.png\" width=900/>\n",
    "<img src=\"e.png\" width=900/>\n",
    "<img src=\"f.png\" width=900/>\n",
    "<img src=\"g.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431faf0-aeee-4750-bf6e-f5b4c40154f8",
   "metadata": {},
   "source": [
    "## üß© Custom Synthetic Data\n",
    "Using Gaussian blobs, we generate random clusters to mimic real data.\n",
    "- Perfect for visualizing how KNN forms boundaries.\n",
    "- Perfect to test overfitting (small K) vs. underfitting (large K).\n",
    "\n",
    "## Data Generation and Preprocessing\n",
    "1. Define function create_realistic_data():\n",
    "   a. Set cluster properties (position, number of samples, standard deviation)\n",
    "   b. For each cluster:\n",
    "      - Generate x and y values using normal distribution\n",
    "      - Combine x and y values into 2D points\n",
    "      - Assign cluster ID to each point\n",
    "   c. Combine all cluster data\n",
    "   d. Return features (X) and labels (y)\n",
    "\n",
    "2. Call create_realistic_data() to generate X and y\n",
    "\n",
    "## Visualization Functions\n",
    "1. Define function plot_knn_boundary(knn, X, y, title):\n",
    "   a. Create a figure\n",
    "   b. Generate a mesh grid for the plot area\n",
    "   c. Predict classes for each point in the mesh grid\n",
    "   d. Create a custom colormap\n",
    "   e. Plot decision regions using contourf\n",
    "   f. Plot original data points with scatter\n",
    "   g. Add title, labels, and colorbar\n",
    "   h. Display the plot\n",
    "\n",
    "2. Define function plot_accuracy_curve(X, y, max_k):\n",
    "   a. Split data into training and testing sets\n",
    "   b. For k from 1 to max_k:\n",
    "      - Create and train KNN model\n",
    "      - Make predictions on test set\n",
    "      - Calculate and store accuracy\n",
    "   c. Plot k values vs accuracies\n",
    "   d. Add title, labels, and grid\n",
    "   e. Display the plot\n",
    "\n",
    "\n",
    "<img src=\"h.png\" width=900/>\n",
    "<img src=\"i.png\" width=900/>\n",
    "<img src=\"j.png\" width=900/>\n",
    "<img src=\"k.png\" width=900/>\n",
    "<img src=\"l.png\" width=900/>\n",
    "<img src=\"n.png\" width=900/>\n",
    "<img src=\"o.png\" width=900/>\n",
    "<img src=\"p.png\" width=900/>\n",
    "<img src=\"q.png\" width=900/>\n",
    "<img src=\"r.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855e4a3-6c1e-4be8-b92d-e1833841fcb8",
   "metadata": {},
   "source": [
    "## üåê Wikipedia + OpenAI Setup\n",
    "- We create a `wiki` client to talk to Wikipedia.\n",
    "- We retrieve the **page content** of each article.\n",
    "- Then we **embed** that text with `get_embedding`.\n",
    "\n",
    "\n",
    "üåå Wikipedia Semantic Explorer Mission: Become an AI librarian finding related articles!\n",
    "\n",
    "üîë API Setup Unlock the knowledge vaults\n",
    "\n",
    "Let's set up the OpenAI API and Wikipedia client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a12dfa-ebcf-4bb3-bf5c-bcd1b5688d83",
   "metadata": {},
   "source": [
    "<img src=\"ss.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca7e7d-96a2-4fcb-9ae0-6a694e55a50b",
   "metadata": {},
   "source": [
    "## üìú `get_article_text` Function üìú\n",
    "1. Connects to **Wikipedia** using `wiki.page`.\n",
    "2. **Checks** if the page exists.\n",
    "3. **Returns** the page‚Äôs text for embedding or analysis.\n",
    "\n",
    "üìö Fetch Wikipedia Articles\n",
    "Implement a function to fetch article content.\n",
    "\n",
    "## Setup and Initialization\n",
    "1. Initialize Wikipedia API with user agent\n",
    "2. Define list of article titles to fetch\n",
    "\n",
    "## Article Fetching and Embedding\n",
    "\n",
    "### Function: get_article_text(title)\n",
    "1. Fetch Wikipedia page for given title\n",
    "2. If page exists, return page text\n",
    "3. Otherwise, return None\n",
    "\n",
    "### Function: get_embedding(text, model)\n",
    "1. Truncate text to 8000 characters\n",
    "2. Count tokens in truncated text\n",
    "3. Update total token count for cost tracking\n",
    "4. Generate embedding using OpenAI API\n",
    "5. Return embedding vector\n",
    "\n",
    "### Main Embedding Process\n",
    "1. Initialize empty lists for embeddings and labels\n",
    "2. For each article title:\n",
    "   a. Fetch article text\n",
    "   b. If text is retrieved:\n",
    "      - Generate embedding\n",
    "      - Add embedding to embeddings list\n",
    "      - Add title to labels list\n",
    "      - Print success message\n",
    "   c. If fetch fails, print failure message\n",
    "\n",
    "## KNN Implementation\n",
    "\n",
    "### KNN Class Initialization\n",
    "1. Store embeddings as numpy array\n",
    "2. Normalize embeddings\n",
    "3. Store labels\n",
    "\n",
    "### KNN Query Method\n",
    "1. Generate embedding for input text\n",
    "2. Normalize query embedding\n",
    "3. Calculate similarities using dot product\n",
    "4. Find indices of top k similar articles\n",
    "5. Return list of (label, similarity) pairs for top matches\n",
    "\n",
    "## KNN Usage\n",
    "1. Initialize KNN with generated embeddings and labels\n",
    "2. (Ready for querying with new text inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa19b95-5128-4ef8-bcc7-da0ea761697f",
   "metadata": {},
   "source": [
    "<img src=\"SA.png\" width=900/>\n",
    "<img src=\"SB.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c62ba-bfaf-4f9d-8734-92da5ebb722c",
   "metadata": {},
   "source": [
    "## üîó `KNN` Class for Wikipedia Articles üîó\n",
    "1. **Holds** embeddings and article labels/titles.\n",
    "2. On **query**, it:\n",
    "   - Embeds your query text.\n",
    "   - Computes **cosine similarity** with all stored article embeddings.\n",
    "   - Sorts by similarity and returns the top matches.\n",
    "\n",
    "üìö Build the KNN Class\n",
    "\n",
    "Implement a KNN class to find similar articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac808a-29c8-452c-9625-7aed3d4378c3",
   "metadata": {},
   "source": [
    "<img src=\"SC.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec28886-46b5-439a-81cc-be297724e472",
   "metadata": {},
   "source": [
    "## Example 1: Finding Similar Articles\n",
    "\n",
    "1. Define query text: \"Neural networks in machine learning\"\n",
    "2. Use KNN to find top 3 similar articles:\n",
    "   a. Call knn.query() with query text and k=3\n",
    "   b. Store results\n",
    "\n",
    "3. Print \"Top Matches:\"\n",
    "4. For each result (title and score):\n",
    "   a. Print title and formatted score\n",
    "\n",
    "## Example 2: Comparing Two Specific Articles\n",
    "\n",
    "1. Define two article titles:\n",
    "   - article1 = \"Artificial Intelligence\"\n",
    "   - article2 = \"Quantum Computing\"\n",
    "\n",
    "2. For each article:\n",
    "   a. Fetch article text using get_article_text()\n",
    "   b. Generate embedding using get_embedding()\n",
    "\n",
    "3. Calculate similarity:\n",
    "   a. Normalize both embeddings\n",
    "   b. Compute dot product of normalized embeddings\n",
    "\n",
    "4. Print similarity score between the two articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fb6e7-21cb-4737-8cb1-f30723e74171",
   "metadata": {},
   "source": [
    "<img src=\"SE.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71901a9-d17e-409e-bef9-1ff85a56071a",
   "metadata": {},
   "source": [
    "üìö Plot Similarity Heatmap\n",
    "\n",
    "Visualize the similarity matrix.\n",
    "\n",
    "## Create Similarity Matrix\n",
    "\n",
    "1. Calculate dot product of embeddings with their transpose:\n",
    "   similarity_matrix = dot_product(embeddings, transpose(embeddings))\n",
    "\n",
    "## Visualize Similarity Matrix as Heatmap\n",
    "\n",
    "1. Create a new figure with size 10x8\n",
    "\n",
    "2. Generate heatmap:\n",
    "   a. Use imshow() to display similarity matrix\n",
    "   b. Set colormap to \"viridis\"\n",
    "\n",
    "3. Set x-axis labels:\n",
    "   a. Use article titles as labels\n",
    "   b. Set tick positions to range from 0 to number of articles\n",
    "   c. Rotate labels 90 degrees\n",
    "\n",
    "4. Set y-axis labels:\n",
    "   a. Use article titles as labels\n",
    "   b. Set tick positions to range from 0 to number of articles\n",
    "\n",
    "5. Add colorbar:\n",
    "   a. Label it \"Cosine Similarity\"\n",
    "\n",
    "6. Set title of the plot to \"Wikipedia Article Similarities\"\n",
    "\n",
    "7. Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d25d8c-14ce-4a40-b77b-8b3dd392d8a6",
   "metadata": {},
   "source": [
    "<img src=\"SF.png\" width=900/>\n",
    "<img src=\"SD.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c2f5c-40cd-489c-a2c9-8cee68da945e",
   "metadata": {},
   "source": [
    "## Takeaway: KNN's Strength and Simplicity\n",
    "\n",
    "Implementing k-Nearest Neighbors was a great exercise to reinforce my understanding of how classification algorithms work. The KNN algorithm is incredibly intuitive because it relies on proximity and majority voting to classify data.\n",
    "\n",
    "Here's what I learned from implementing KNN:\n",
    "\n",
    "1. KNN is easy to understand and implement: It‚Äôs based on simple distance calculations and majority voting. This makes it one of the simplest algorithms to grasp, especially for beginners.\n",
    "   \n",
    "2. The value of distance metrics: The Euclidean distance formula, which is used to calculate the proximity between data points, plays a crucial role in determining which points are most similar. You can experiment with different distance metrics for different types of data.\n",
    "\n",
    "3. Performance considerations: While KNN is a great algorithm, it can become slow when working with large datasets because it calculates the distance to every single training point for each test sample. For large datasets, optimizations like KD-Trees or Ball Trees can be used to speed up the process.\n",
    "\n",
    "4. Choosing the right value for K: The performance of KNN depends heavily on the value of k. A small k can be sensitive to noise, while a large k might smooth out the classification too much. Tuning k is essential for improving the model's accuracy.\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Overall, implementing KNN was a great learning experience. Despite being a simple algorithm, it provides a strong foundation for understanding the basics of classification tasks. It helped me appreciate how distance-based classification works and how we can use it to classify new, unseen data based on patterns in the training set.\n",
    "\n",
    "In future projects, I‚Äôll experiment with different distance metrics, fine-tune the value of k, and possibly look into more advanced techniques to improve the efficiency of KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c5e2a-01e2-4bd7-bf99-6c601982dae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
