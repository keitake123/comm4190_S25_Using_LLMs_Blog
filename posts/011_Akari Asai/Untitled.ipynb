{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5ac192be-fcf1-4749-bbf6-960e57a09160",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Akari Asai's job talk on LLMs+RAG\"\n",
    "description: \"Bridging Informal and Formal AI Reasoning\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - RAG\n",
    "  - Talk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0896b2-f026-49ee-838d-9d5537af3203",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "On Monday, February 10, I had the opportunity to attend a virtual talk by Akari Asai, a PhD student at the University of Washington. Her presentation focused on retrieval-augmented language models (RAG), which aim to enhance traditional language models by integrating external knowledge sources for more accurate and reliable responses. Unlike traditional monolithic models that rely solely on internal training data, RAG models use retrieval mechanisms to bring in real-time or large-scale information to provide more nuanced answers. This method addresses some of the main challenges of language models, such as generating inaccurate information or struggling to cover less common topics.\n",
    "\n",
    "The talk covered Self-Reflect and Correct (Self-RAG), a framework that combines planning and generation into one continuous process. She also presented Open Scatter, an open-source tool designed to help researchers synthesize scientific literature more efficiently. Akari Asai’s work in this field is particularly significant as it demonstrates how AI systems can evolve beyond static models to be more adaptive and contextually aware.\n",
    "\n",
    "As someone passionate about AI and its potential applications, I found this talk incredibly insightful. It provided both theoretical and practical perspectives on improving language models through augmentation and human feedback, while also addressing key limitations in current models.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"akari-asai-blog-main.jpg\" width=400/>\n",
    "\n",
    "https://akariasai.github.io/\n",
    "\n",
    "https://drive.google.com/file/d/1uBpmAdxTRhtCi5PqoFA2jThjYNWVWou2/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d579a-d9dc-4c83-9b85-793c46a2b8c2",
   "metadata": {},
   "source": [
    "Summary of the talk\n",
    "Ms. Akari Asai's presentation discusses augmented language models as a way to improve the efficiency and reliability of language models. Retrieval-augmented language models enhance language models by incorporating large-scale text data and Ms. Asai addresses using these to combat challenges such as generating inaccurate information and struggling with less common topics. She addresses three key questions: Why augment language models rather than continuing to scale monolithic ones? How should the foundations of retrieval-augmented models be built? What can be achieved with state-of-the-art models to make a real-world impact?\n",
    "\n",
    "Ms. Asai introduces Self-Reflect and Correct (Self-RAG), a model that integrates planning and generation into a single process. The training of Self-RAG involves a three-stage process that includes training a critical language model to guide the generative language mode. As a practical application of augmented language models, she presented Open Scatter, a fully open-source model designed to assist scientists in synthesizing scientific literature.\n",
    "\n",
    "Class do you think they should teach, if they come to Penn\n",
    "This could be an exiting class, or a new one that they design.\n",
    "Akari Asai is working on large language models and retrieval-augmented generation, which is a significant technique for improving these models. Her focus on addressing limitations and her research on improving this area is definitely something that a lot of students like me are interested in. Also since I am also Japanese like her, I would love more Japanese faculty on campus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2749b05-29b6-4d04-b454-6f06b6d65e63",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Attending Akari Asai’s talk on retrieval-augmented language models was an enriching experience, and here’s what I took away from it:\n",
    "\n",
    "1. Why augment language models: Akari made a compelling case for moving beyond traditional, monolithic models. By using retrieval mechanisms, RAG models can incorporate real-time external information, allowing them to handle more complex queries and provide contextually rich responses. This is essential for improving the accuracy and reliability of language models, especially in areas where existing models struggle.\n",
    "\n",
    "2. Self-RAG and planning-generation integration: The introduction of Self-RAG was one of the most innovative aspects of the talk. By combining planning and generation, this framework allows models to reflect on and correct their responses during the generation process, creating more dynamic and interactive models. This level of self-reflection could lead to significant improvements in how AI systems adapt to human feedback.\n",
    "\n",
    "3. Practical applications with Open Scatter: One of the most exciting applications discussed was Open Scatter, an open-source project designed to help scientists synthesize scientific literature. This demonstrated how RAG models can be applied to real-world tasks such as research, where efficient knowledge retrieval and summarization are essential. It highlighted the tangible impact of RAG models in academic and professional fields.\n",
    "\n",
    "4. Human feedback for model refinement: Akari also discussed how human feedback plays a crucial role in refining RAG models. The integration of Reinforcement Learning from Human Feedback (RLHF) allows models to evolve based on user preferences, leading to more accurate and personalized outputs. This is a significant step forward in creating AI systems that are more aligned with human values.\n",
    "\n",
    "5. Challenges and open questions: Despite the progress made in RAG, Akari raised several open questions in the field, such as whether current RLHF techniques can fully address the issue of Overton pluralism, which deals with representing all reasonable perspectives on a query. She also mentioned challenges related to scalability and efficiency, highlighting the need for further research to optimize these models for large-scale applications.\n",
    "\n",
    "Overall, Akari Asai’s talk provided a detailed look into the future of AI systems that go beyond simple data generation, incorporating real-time external knowledge and human feedback to make AI more context-aware and adaptive. Her work on RAG models has the potential to revolutionize how we think about language models and their applications in real-world tasks.\n",
    "\n",
    "I look forward to following her research and seeing how RAG models continue to evolve. I’m also hopeful that this work will pave the way for more inclusive and contextually intelligent AI systems in the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e43e10-6711-491d-8ff5-c5bfb3e33c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
