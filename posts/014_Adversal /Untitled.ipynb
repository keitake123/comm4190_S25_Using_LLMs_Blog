{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7100c-827f-4eca-b222-45d111e61afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Akari Asai's job talk on LLMs+RAG\"\n",
    "description: \"Bridging Informal and Formal AI Reasoning\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - RAG\n",
    "  - Talk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2fe59-76bb-43f7-a86f-eafb7fbf6b5b",
   "metadata": {},
   "source": [
    "### USER\n",
    "_Can you craft an adversarial example for an MNIST sample?_\n",
    "\n",
    "### ASSISTANT\n",
    "Crafting an adversarial example for an MNIST sample involves making subtle modifications to an image of a digit such that a machine learning model misclassifies it while humans still recognize it as the original digit. Here's a simplified explanation of how you could create such an adversarial example:\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **Trained Model**: You need a machine learning model trained on the MNIST dataset. This could be a convolutional neural network (CNN), which is commonly used for image classification tasks.\n",
    "\n",
    "2. **FGSM (Fast Gradient Sign Method)**: One of the simplest and most popular methods for generating adversarial examples.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Choose a Sample**: Pick an MNIST image from the test set that you want to turn into an adversarial example.\n",
    "\n",
    "2. **Compute the Gradient**: Calculate the gradient of the loss with respect to the input image. This measures how changes in the input influence the loss:\n",
    "\n",
    "   \\[\n",
    "   \\text{gradient} = \\nabla_X J(\\theta, X, y)\n",
    "   \\]\n",
    "\n",
    "   Here, \\( J \\) is the loss function, \\( \\theta \\) are the model parameters, \\( X \\) is the input image, and \\( y \\) is the true label.\n",
    "\n",
    "3. **Generate Perturbation**: Create a small perturbation by taking the sign of the computed gradient:\n",
    "\n",
    "   \\[\n",
    "   \\text{perturbation} = \\epsilon \\times \\text{sign}(\\text{gradient})\n",
    "   \\]\n",
    "\n",
    "   The parameter \\(\\epsilon\\) controls the magnitude of the perturbation. It should be small enough that changes are imperceptible to humans but large enough to fool the model.\n",
    "\n",
    "4. **Create Adversarial Example**: Add the perturbation to the original image:\n",
    "\n",
    "   \\[\n",
    "   X_{\\text{adv}} = X + \\text{perturbation}\n",
    "   \\]\n",
    "\n",
    "5. **Clip to Valid Range**: Ensure that pixel values of the adversarial image are still within the valid range [0, 1], which is common for normalized inputs.\n",
    "\n",
    "6. **Test the Model**: Run the adversarial image through the model and observe the output. The goal is for the model to misclassify this image.\n",
    "\n",
    "### Example Code (Using PyTorch)\n",
    "\n",
    "Here's a basic implementation using PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_set = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Assume `model` is already defined and trained\n",
    "model.eval()\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "# Get an example and label\n",
    "data_iter = iter(test_loader)\n",
    "image, label = next(data_iter)\n",
    "\n",
    "# Set requires_grad attribute of tensor\n",
    "image.requires_grad = True\n",
    "\n",
    "# Forward pass\n",
    "output = model(image)\n",
    "loss = nn.CrossEntropyLoss()(output, label)\n",
    "\n",
    "# Zero gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Collect data gradient\n",
    "data_grad = image.grad.data\n",
    "\n",
    "# Call FGSM\n",
    "adv_image = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "# Re-classify the perturbed image\n",
    "output = model(adv_image)\n",
    "\n",
    "# Check for success\n",
    "_, final_pred = torch.max(output, 1)\n",
    "if final_pred.item() != label.item():\n",
    "    print(f\"Attack successful: original label {label.item()}, new prediction {final_pred.item()}\")\n",
    "else:\n",
    "    print(\"Attack failed.\")\n",
    "\n",
    "```\n",
    "\n",
    "### Important Points\n",
    "\n",
    "- **Epsilon Value**: The choice of \\(\\epsilon\\) is crucial. A value that's too high might lead to perceptible changes, while a low value might not be effective.\n",
    "- **Universal Applicability**: Different models may require different \\(\\epsilon\\) values.\n",
    "- **Security Risks**: Adversarial examples pose significant security risks in sensitive applications. It's essential to account for these when designing robust models.\n",
    "\n",
    "This example illustrates a common method for generating adversarial examples, but in practice, creating effective adversarial attacks and ensuring model robustness can be more nuanced.\n",
    "\n",
    "### USER\n",
    "_Write Python code to craft an adversarial example for MNIST digit 5 to be misclassified as digit 8._\n",
    "\n",
    "### ASSISTANT\n",
    "Certainly! Below is a Python implementation using the Fast Gradient Sign Method (FGSM) to generate an adversarial example from an MNIST image of the digit '5', aiming to misclassify it as '8'. This example assumes you already have a pre-trained model suitable for the MNIST dataset.\n",
    "\n",
    "Here's how you can achieve this with PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_set = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Assume `model` is already loaded and trained; set it to evaluation mode\n",
    "model = ...  # Your pre-trained model here\n",
    "model.eval()\n",
    "\n",
    "# Function to perform the FGSM attack\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # Clip the perturbed image to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Select the epsilon value\n",
    "epsilon = 0.25\n",
    "\n",
    "# Look for an image of the digit '5'\n",
    "for image, label in test_loader:\n",
    "    if label.item() == 5:\n",
    "        # Set requires_grad attribute of tensor to track gradient\n",
    "        image.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(image)\n",
    "\n",
    "        # Calculate the loss using the cross-entropy loss function\n",
    "        # We pretend the label is '8' to craft an example to be classified as '8'\n",
    "        target_label = torch.tensor([8])\n",
    "        loss = nn.CrossEntropyLoss()(output, target_label)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backward pass to compute the gradient of the loss w.r.t the input image\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect the computed gradients\n",
    "        data_grad = image.grad.data\n",
    "\n",
    "        # Call FGSM to create the adversarial example\n",
    "        adv_image = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(adv_image)\n",
    "\n",
    "        # Check if it's misclassified as '8'\n",
    "        _, final_pred = torch.max(output, 1)\n",
    "        if final_pred.item() == 8:\n",
    "            print(f\"Successfully converted '5' to be classified as '8'.\")\n",
    "        else:\n",
    "            print(\"Failed to misclassify the '5' as '8'.\")\n",
    "\n",
    "        # Break after one successful example\n",
    "        break\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Model Loading**: You need to load your pre-trained model before using it in the script. Replace `model = ...` with your actual model code.\n",
    "\n",
    "2. **Epsilon Value**: The `epsilon` value is set to `0.25` here, which controls the strength of the perturbation. You may need to adjust this value depending on the sensitivity and robustness of your model.\n",
    "\n",
    "3. **Device Configuration**: This script uses CPU by default. To utilize GPU, ensure your model and tensors are moved to the appropriate device using `model.to(device)`, `image.to(device)`, etc.\n",
    "\n",
    "4. **Stopping Criterion**: The loop exits after successfully crafting one adversarial example that misclassifies '5' as '8'. Adjust this to test with more samples or make modifications.\n",
    "\n",
    "By running this code, you should be able to manipulate a sample of the digit '5' for it to be misclassified as '8' by the model.\n",
    "\n",
    "### USER\n",
    "_Write Python code to demonstrate membership inference in CIFAR-10._\n",
    "\n",
    "### ASSISTANT\n",
    "Membership inference attacks aim to determine whether a particular data point was part of the dataset used to train a machine learning model. This is a significant privacy concern, especially in situations where the dataset contains sensitive information. Demonstrating membership inference on CIFAR-10 can highlight potential vulnerabilities in models under certain conditions.\n",
    "\n",
    "Below is a basic example of how one might set up such an experiment using a white-box approach, assuming access to the model, its parameters, and its outputs. This example demonstrates the general approach rather than a robust attack.\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, ensure that you have a trained model on CIFAR-10 and the necessary dependencies:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "full_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_set_size = int(len(full_dataset) * 0.8)\n",
    "holdout_set_size = len(full_dataset) - train_set_size\n",
    "\n",
    "train_set, holdout_set = random_split(full_dataset, [train_set_size, holdout_set_size])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# Assume we have a pre-trained model\n",
    "# Replace with your model or load a saved model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(2304, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load a pre-trained model (for demonstration, here we assume the model is trained)\n",
    "model = SimpleCNN()\n",
    "# Load your pre-trained weights\n",
    "# model.load_state_dict(torch.load('your_pretrained_model.pth'))\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Simple membership inference using confidence scores\n",
    "def membership_inference_attack(example, model):\n",
    "    with torch.no_grad():\n",
    "        output = model(example)\n",
    "        probs = nn.Softmax(dim=1)(output)\n",
    "        confidence_score, predicted_label = torch.max(probs, 1)\n",
    "        return confidence_score.item(), predicted_label.item()\n",
    "\n",
    "# Compare a sample from the training set and holdout set\n",
    "data_iter_train = iter(DataLoader(train_set, batch_size=1))\n",
    "data_iter_holdout = iter(DataLoader(holdout_set, batch_size=1))\n",
    "\n",
    "train_example, train_label = next(data_iter_train)\n",
    "holdout_example, holdout_label = next(data_iter_holdout)\n",
    "\n",
    "train_confidence, _ = membership_inference_attack(train_example, model)\n",
    "holdout_confidence, _ = membership_inference_attack(holdout_example, model)\n",
    "\n",
    "print(f\"Train example confidence score: {train_confidence:.4f}\")\n",
    "print(f\"Holdout example confidence score: {holdout_confidence:.4f}\")\n",
    "\n",
    "# Simple decision: higher confidence suggests membership, though real attacks require more sophistication\n",
    "if train_confidence > holdout_confidence:\n",
    "    print(\"Membership inference attack suggests the example is part of the training set.\")\n",
    "else:\n",
    "    print(\"Membership inference attack suggests the example is part of the holdout set.\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Data Preparation**: We split the CIFAR-10 training data into a smaller training set and a holdout set.\n",
    "\n",
    "2. **Model**: We use a simple CNN model for demonstration, but typically this would be a more complex pretrained model.\n",
    "\n",
    "3. **Attack Strategy**: The example computes confidence scores for inputs and uses those to infer membership. Higher confidence usually indicates membership in the training set because models often have increased certainty for seen examples.\n",
    "\n",
    "4. **Evaluation**: This demonstration prints confidence levels to provide insight into the decision-making process.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Simplistic Model**: This uses a basic CNN and doesn't represent robust attacks used in practice.\n",
    "- **Effectiveness**: Real membership inference attacks are more sophisticated and often require additional model information and assumptions.\n",
    "- **Ethical Use**: Consider privacy implications and ethical guidelines when utilizing membership inference techniques.\n",
    "\n",
    "To make this more robust, you'd integrate techniques like thresholding, adversarial training, or differential privacy measures in your actual implementations.\n",
    "\n",
    "### USER\n",
    "_Write Python code that poisons 5% of the MNIST dataset._\n",
    "\n",
    "### ASSISTANT\n",
    "Poisoning a dataset involves intentionally altering a subset of the data to mislead the learning process of a machine learning model. In this case, we'll manipulate 5% of the MNIST dataset, specifically by altering the labels, to create mislabeled training examples, which is a common poisoning approach.\n",
    "\n",
    "Below is a simple code snippet that demonstrates how to poison 5% of the MNIST dataset by randomly changing the labels of a selected subset:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Convert the dataset to a list for easy manipulation\n",
    "train_data = list(train_set)\n",
    "\n",
    "# Set the poisoning percentage and compute the number of samples to poison\n",
    "poison_percentage = 0.05\n",
    "num_poison_samples = int(len(train_data) * poison_percentage)\n",
    "\n",
    "# Select random indices for poisoning\n",
    "poison_indices = random.sample(range(len(train_data)), num_poison_samples)\n",
    "\n",
    "# Poison the dataset by changing the label of the selected indices\n",
    "for idx in poison_indices:\n",
    "    image, true_label = train_data[idx]\n",
    "    new_label = true_label\n",
    "\n",
    "    # Ensure the new label is different from the true label\n",
    "    while new_label == true_label:\n",
    "        new_label = random.randint(0, 9)\n",
    "\n",
    "    train_data[idx] = (image, new_label)\n",
    "\n",
    "# Create a poisoned DataLoader\n",
    "poisoned_train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Example of checking a few poisoned samples\n",
    "print(\"Example poisoned data points (original labels replaced):\")\n",
    "for i, (image, label) in enumerate(poisoned_train_loader):\n",
    "    if i == 5:  # Check first 5 batches\n",
    "        break\n",
    "    print(f\"Batch {i+1}, Labels: {label.tolist()}\")\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Data Conversion**: Convert the `train_set` to a list to manipulate the data easily, accessing images and labels by index.\n",
    "\n",
    "2. **Random Selection**: Use random sampling to select a proportion of the dataset to poison. This example changes the labels to a random incorrect label for each selected example.\n",
    "\n",
    "3. **Ensuring Different Labels**: Ensures that poisoned labels are indeed different from the original labels to effectively poison the dataset.\n",
    "\n",
    "4. **Poisoned DataLoader**: The poisoned dataset is wrapped in a `DataLoader` for further training or evaluation.\n",
    "\n",
    "5. **Ethical Considerations**: Dataset poisoning should be handled responsibly and ethically, especially in research or environments where the integrity of data is critical.\n",
    "\n",
    "This simplistic example focuses on manipulating labels; however, other poisoning strategies include altering image data directly to introduce model vulnerabilities subtly. Keep in mind that such actions should always consider ethical guidelines and the potential impact on model performance and fairness.\n",
    "\n",
    "### USER\n",
    "_With 20% of CIFAR10 dataset, write Python code to steal an AlexNet model trained on CIFAR10_\n",
    "\n",
    "### ASSISTANT\n",
    "Model stealing attacks involve using queries to a target model to approximate its functionality and replicate its decision-making process. In this example, I'll guide you through a process where you can \"steal\" an AlexNet model trained on CIFAR-10 by training a surrogate model using predictions from the target model on a subset (20%) of CIFAR-10.\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, ensure you have the necessary dependencies. If you don't have them installed, you can install them using:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Below is an example of how you can approach stealing a model:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import alexnet\n",
    "import random\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # AlexNet requires 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "full_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Use 20% of the dataset for the attack\n",
    "subset_size = int(len(full_dataset) * 0.2)\n",
    "random_indices = random.sample(range(len(full_dataset)), subset_size)\n",
    "subset = Subset(full_dataset, random_indices)\n",
    "subset_loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Assume `target_model` is the AlexNet model trained on CIFAR-10\n",
    "target_model = alexnet(pretrained=False)\n",
    "# Load trained weights (for demonstration purposes, use random weights or your trained model)\n",
    "# target_model.load_state_dict(torch.load('your_trained_alexnet.pth'))\n",
    "target_model.eval()\n",
    "\n",
    "# Define a surrogate model (same architecture for easier comparison)\n",
    "surrogate_model = alexnet(pretrained=False)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_model.to(device)\n",
    "surrogate_model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(surrogate_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for the surrogate model\n",
    "epochs = 5\n",
    "surrogate_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, _ in subset_loader:\n",
    "        # Move inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Get predictions from the target model (soft labels)\n",
    "        with torch.no_grad():\n",
    "            soft_labels = target_model(inputs)\n",
    "\n",
    "        # Forward pass through the surrogate model\n",
    "        outputs = surrogate_model(inputs)\n",
    "        loss = criterion(outputs, torch.argmax(soft_labels, dim=1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Surrogate model training complete.\")\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Model Selection**: Both the target and surrogate models use AlexNet architecture, which is better suited for ImageNet-sized inputs. Therefore, CIFAR-10 inputs must be resized to 224x224.\n",
    "\n",
    "2. **Data Subsetting**: We use 20% of the CIFAR-10 dataset by randomly selecting indices. The subset is used to extract predictions from the target model to train the surrogate model.\n",
    "\n",
    "3. **Stealing Process**: The surrogate model is trained on inputs and the soft labels (predictions) from the target model.\n",
    "\n",
    "4. **Soft Labels vs. Hard Labels**: The soft labels from the target model's output help guide the surrogate model's learning, providing richer information than hard labels alone.\n",
    "\n",
    "5. **Ethical Concerns**: Model stealing can infringe on intellectual property rights. This example is purely academic and should be used responsibly, with due consideration for legal and ethical guidelines.\n",
    "\n",
    "6. **GPU Utilization**: The model training process may require substantial computational power, so ensuring the code runs on CUDA-capable GPUs (if available) can speed up the process significantly.\n",
    "\n",
    "This code provides a framework for understanding model extraction techniques. Further sophistication might involve probing techniques, architecture differences, or using a different model architecture for the surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c144042-800d-4be7-83a3-6adc751f2509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
