{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3c5a67c1-b141-4eac-b086-38375703637e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Anomaly Detection in Time Series using ChatGPT\"\n",
    "description: \"How to explore and evaluate a data analysis topic with an automated conversational framework?\n",
    "\"\n",
    "author: \"Kei Taketsuna\"\n",
    "date: \"3/23/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Informal Reasoning\n",
    "  - Formal Reasoning\n",
    "  - Talk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f1cec-4322-4eea-b3ec-1b206940c4d8",
   "metadata": {},
   "source": [
    "This is based on a post on the Medium that I found really intresting. <br>\n",
    "https://medium.com/@sztistvan/anomaly-detection-in-time-series-using-chatgpt-3fc48f958c88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3f157-a429-4f42-aa48-15ce8cbbfdd7",
   "metadata": {},
   "source": [
    "```\n",
    "# **Example**\n",
    "\n",
    "> **_QUERY_**_: Create an example to demonstrate anomaly detection in time series data. Use Python without any specific package._\n",
    "_Remark_: When I posed this question across different sessions, responses varied, sometimes using specific packages such as Prophet, and sometimes sticking only to packages like NumPy and Pandas. Hence, I made it clear not to employ additional packages.\n",
    "Here’s the response I received:\n",
    "\n",
    "> **_ChatGPT_**_: For this example, I’ll use the _Numenta Anomaly Benchmark (NAB)_ dataset, which offers various real-world time series data with labeled anomalies. Specifically, I’ll utilize the “machine_temperature_system_failure.csv” dataset, which contains temperature readings from a machine in an industrial context._\n",
    "\n",
    "> _I’ll perform anomaly detection on this dataset using a simple statistical approach called the _Moving Average (MA)_ technique. This technique computes the moving average of the data over a given window and detects anomalies via deviations from this moving average._\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/machine_temperature_system_failure.csv')\n",
    "# Convert the timestamp column to a datetime object\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# Compute the moving average of the temperature readings\n",
    "window_size = 200 # MODIFICATION, originally 50\n",
    "ma = df['value'].rolling(window_size).mean()\n",
    "# Calculate the deviation from the moving average\n",
    "deviation = df['value'] - ma\n",
    "# Calculate the standard deviation of the deviation\n",
    "std_deviation = deviation.rolling(window_size).std()\n",
    "# Determine the threshold for anomaly detection\n",
    "threshold = 3 * std_deviation\n",
    "# Identify anomalies based on deviations from the moving average\n",
    "anomalies = df[deviation.abs() > threshold]\n",
    "# Plot the temperature readings and the anomalies\n",
    "plt.subplots(figsize=(14, 10)) # MODIFICATION, inserted\n",
    "plt.plot(df['timestamp'], df['value'], color='blue', label='Temperature Readings')\n",
    "plt.scatter(anomalies['timestamp'], anomalies['value'], color='red', label='Anomalies')\n",
    "plt.plot(df['timestamp'], ma, color='green', label='Moving Average')\n",
    "plt.fill_between(df['timestamp'], ma-threshold, ma+threshold, color='gray', alpha=0.2, label='Threshold')\n",
    "plt.legend()\n",
    "plt.title('Machine Temperature Anomaly Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Celsius)')\n",
    "plt.grid() # MODIFICATION, inserted\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a661a1b-d1ae-4657-9c1f-8273d9ee9390",
   "metadata": {},
   "source": [
    "Remark: I made simple modifications, adjusting the time window parameter, and added lines to improve plot visuals. <br>\n",
    "\n",
    "This basic anomaly detection yields several insights:<br>\n",
    "<br>\n",
    "We accessed real-world data using the NAB dataset. You can obtain information about this dataset through ChatGPT by asking: “What is the Numenta Anomaly Benchmark (NAB) dataset and how can I access it?” or search sites like Google and Kaggle for examples and studies.\n",
    "We employed a straightforward approach using a moving average calculated with a moving window operator and set thresholds based on data deviations from this average.\n",
    "We added visualization enhancements, such as clearly displaying the anomaly detection threshold with shaded gray regions.\n",
    "Despite the simplicity, it was achieved quickly.\n",
    "Although the results weren't optimal, this issue is manageable at this stage. We can delve into parameter tuning:\n",
    "Asking about setting the parameters\n",
    "It's clear that this is a commonly accepted rule of thumb in statistics and data assessment for defining thresholds, especially in identifying extreme values or outliers. Let's explore some variations.\n",
    "\n",
    "## Z-score\n",
    "To further enhance detection, we might calculate deviations from the mean and compare these to the standard deviation, essentially applying the Z-score. This Z-score shows how many standard deviations a given value is from the mean. By adjusting the code, we can approach it thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5ebd5-b6f3-462e-aa33-9a621625d67d",
   "metadata": {},
   "source": [
    "```\n",
    "# Calculate the average of the temperature readings\n",
    "mean = df['value'].mean()\n",
    "# Compute the deviation from the average\n",
    "deviation = df['value'] - mean\n",
    "# Determine the standard deviation of the deviation\n",
    "std_deviation = deviation.std() # equivalent to df['value'].std()\n",
    "# Compute the Z-score - for reference but not directly used\n",
    "z_score = deviation / std_deviation # z_score = (value-mean)/std\n",
    "# Establish the threshold for anomaly detection\n",
    "threshold = 3 * std_deviation\n",
    "# Identify anomalies based on deviations from the moving average\n",
    "anomalies = df[deviation.abs() > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3bfa90-feb8-4f45-b3fc-5bbbfe2d3333",
   "metadata": {},
   "source": [
    "Anomaly detection founded on Z-score\n",
    "\n",
    "## Percentiles <br>\n",
    "We can define global upper and lower limits, leveraging the percentage distribution of the data values. Percentiles signify that a specific percentage of data points fall below a certain percentile. For instance, the 50th percentile represents the median. To define lower and upper bounds encompassing 99.7% of the data, apply the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6f223-edb9-4f25-be2e-c6d1c91574e2",
   "metadata": {},
   "source": [
    "```\n",
    "# calculate percentile limits\n",
    "percent_limit = 0.3 # 99.7% is set\n",
    "upper_threshold = df['value'].quantile((100-percent_limit/2)/100) \n",
    "lower_threshold = df['value'].quantile(percent_limit/2/100) \n",
    "# Detect anomalies based on percentile limits\n",
    "anomalies_upper = df[df['value'] > upper_threshold]\n",
    "anomalies_lower = df[df['value'] < lower_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bed4e7-9bbb-4cce-80cc-a6054bfe638f",
   "metadata": {},
   "source": [
    "\n",
    "## Anomaly detection using percentiles\n",
    "\n",
    "It's crucial now to clarify the concept of an anomaly.\n",
    "\n",
    "## Anomaly Detection\n",
    "We immediately dive into differentiating between anomaly detection and change point detection:\n",
    "\n",
    "Anomaly detection in time series:\n",
    "\n",
    "Anomalies typically signify potential issues in the underlying system or process generating the data.\n",
    "It involves pinpointing data points or occurrences significantly deviating from anticipated or typical series behavior.\n",
    "Change point detection in time series:\n",
    "\n",
    "Change points typically represent notable transitions in the underpinning system or process producing the data.\n",
    "This involves identifying time points where the series' statistical attributes abruptly alter.\n",
    "The crux is that anomaly detection zooms in on individual points or events deviating from expected norms, whereas change point detection focuses on temporal points where the series' statistical facets shift.\n",
    "\n",
    "A straightforward way to remember:\n",
    "\n",
    "Persistent anomalies are change points. \n",
    "\n",
    "Consider monitoring system measurements as a continuous series. Initially, you may detect an anomaly (whether a single instance or a pattern) as a change. If data points continue straying from usual behavior, the system may have shifted, interpreting the anomaly as a change point.\n",
    "\n",
    "Types of Anomalies\n",
    "A pertinent discussion involves types of anomalies. Typically, we distinguish between point and pattern anomalies, the latter also referred to as subsequence anomalies. Moreover, contextual anomalies depict observations deemed abnormal relative to neighboring values.\n",
    "\n",
    "QUERY: What types of anomalies are in time series?\n",
    "\n",
    "ChatGPT: In time series analysis, several types of anomalies can arise, including:\n",
    "\n",
    "1. Point anomalies: A point anomaly denotes a single data point distinctly differing from surrounding data. Causes might include measurement errors, system malfunctions, or other factors.\n",
    "\n",
    "2. Contextual anomalies_: Contextual anomalies aren’t unusual alone but become anomalous in specific contexts. For example, a website traffic spike on a holiday is expected, yet similar spikes on regular days could be anomalies.\n",
    "\n",
    "3. Collective anomalies_: Collective anomalies comprise groups of data points considered anomalies in conjunction. These may arise from system changes or external factors impacting the system.\n",
    "\n",
    "4. Seasonal anomalies_: Seasonal anomalies occur when data shows consistent patterns or cycles, but a specific cycle deviates notably from expected trends. Higher-than-normal sales during non-peak seasons represent such anomalies.\n",
    "\n",
    "5. Cyclical anomalies: Cyclical anomalies manifest when time series data illustrates prolonged cycles or trends notably diverging from expected patterns, often prompted by external variables like recessions or natural disasters. \n",
    "\n",
    "Remark: In the context of collective anomalies, the notion of pattern anomalies is evident.\n",
    "\n",
    "With these concepts clarified, we generated an example to illustrate both anomalies and change points:\n",
    "Time series with both change points and anomalies focus\n",
    "\n",
    "Algorithms for Anomaly Detection\n",
    "So far, we've explored applying a simple moving average approach and touched on statistical methods. Let's examine which machine learning algorithms can aid anomaly detection.\n",
    "\n",
    "QUERY: What specific machine learning algorithms are for anomaly detection? \n",
    "\n",
    "Summarizing various responses across sessions:\n",
    "\n",
    "ChatGPT: Numerous machine learning algorithms exist for anomaly detection in time series.\n",
    "\n",
    "1. Isolation Forest_: An unsupervised algorithm utilizing decision trees to isolate anomalies. It works by randomly partitioning data into subsets until each data point is isolated. Fewer partitions indicate higher anomaly likelihood.\n",
    "\n",
    "2. Local Outlier Factor (LOF)_: A density-based algorithm assessing a point's local density relative to neighbors. Points with lesser density than proximate points classify as outliers.\n",
    "\n",
    "3. One-Class Support Vector Machine (SVM)_: This SVM variation is trained on just one data class, learning a boundary separating training data from the origin, considering any point crossing this boundary as an anomaly.\n",
    "\n",
    "4. LSTM-based models_: LSTM, a type of Recurrent Neural Network (RNN), excels in modeling sequences, aiding anomaly detection by learning intricate patterns over time.\n",
    "\n",
    "5. Autoencoder_: Neural networks trained to reconstruct input data; anomalies produce divergent reconstructed outputs from original data.\n",
    "\n",
    "6. ARIMA (AutoRegressive Integrated Moving Average)__: Primarily for forecasting, ARIMA extends to anomaly detection by comparing predicted and actual values.\n",
    "\n",
    "7. Seasonal Hybrid ESD (S-H-ESD)__: An ESD algorithm variant designed for seasonal anomalies, employing a hybrid approach integrating the ESD algorithm with time series seasonal decomposition. \n",
    "\n",
    "Isolation Forest\n",
    "Isolation Forest represents an unsupervised machine learning approach utilizing decision tree ensembles to isolate anomalies within data.\n",
    "\n",
    "QUERY: Create a python function applying the Isolation Forest algorithm to detect time series anomalies. The function input is a Pandas series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351dc4d3-ed3e-4bb0-a6d0-c39a77caf473",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "def detect_anomalies_with_isolation_forest(series):\n",
    "    # Convert the series to a 2D NumPy array\n",
    "    data = series.values.reshape(-1, 1)\n",
    "    \n",
    "    # Create an instance of the IsolationForest class\n",
    "    #model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "    model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "    # Fit the model to the data and predict anomalies\n",
    "    model.fit(data)\n",
    "    anomalies = model.predict(data)\n",
    "    \n",
    "    # Convert the predictions back to a Pandas series and return it\n",
    "    anomalies_series = pd.Series(anomalies, index=series.index)\n",
    "    return anomalies_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4dd20d-9975-4c92-b46b-aa3332e3be68",
   "metadata": {},
   "source": [
    "Plotting the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7c4c3-c650-4526-85c1-59b979a255b2",
   "metadata": {},
   "source": [
    "```\n",
    "# Set the timestamp column as the index and convert to a series\n",
    "series = df.set_index('timestamp')['value'].squeeze()\n",
    "# Detect anomalies using the Isolation Forest algorithm\n",
    "anomalies = detect_anomalies_with_isolation_forest(series)\n",
    "# Plot the original series and the detected anomalies\n",
    "plt.subplots(figsize=(14, 10)) \n",
    "plt.plot(df['timestamp'], df['value'], color='blue', label='Temperature Readings')\n",
    "plt.scatter(anomalies[anomalies==-1].index, series[anomalies==-1].values, color='red', label='Anomalies')\n",
    "plt.legend()\n",
    "plt.title('Machine Temperature Anomaly Detection - Isolation Forest')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Celsius)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec3478-7154-481d-854a-33b2d21b1e2b",
   "metadata": {},
   "source": [
    "By replacing the parameter contamination ‘auto‘ with a value of 0.1 and observing the results: <br>\n",
    "Isolation Forest example<br>\n",
    "Unlike using the Z-score method, detected points align more closely with local irregularities.\n",
    "\n",
    "Querying for deeper Isolation Forest parameter explanations:<br>\n",
    "\n",
    "Local Outlier Factor<br>\n",
    "Local Outlier Factor (LOF) is an unsupervised anomaly detection algorithm gauging a point's local density relative to neighbors, based on the premise that anomalies naturally reside in low-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f96552-6b1e-4d2d-9d63-64ab8d21126f",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def detect_anomalies_with_local_outlier(series):\n",
    "    #lof = LocalOutlierFactor(n_neighbors=10, contamination='auto')\n",
    "    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.01)\n",
    "    X = series.values.reshape(-1,1)\n",
    "    y_pred = lof.fit_predict(X)\n",
    "    anomalies = X[y_pred==-1]\n",
    "    return pd.Series(anomalies.flatten(), index=series.index[y_pred==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2da8d1-06d9-44fd-847c-09123e1cdf80",
   "metadata": {},
   "source": [
    "```\n",
    "# Detect anomalies using the Isolation Forest algorithm\n",
    "anomalies = detect_anomalies_with_local_outlier(series)\n",
    "# Plot the original series and the detected anomalies\n",
    "plt.subplots(figsize=(14, 10)) \n",
    "plt.plot(df['timestamp'], df['value'], color='blue', label='Temperature Readings')\n",
    "plt.scatter(anomalies.index, anomalies.values, color='red', label='Anomalies')\n",
    "plt.legend()\n",
    "plt.title('Machine Temperature Anomaly Detection - Local Outlier Factor')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Celsius)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca875b-8156-4610-9e3e-6733b4be7433",
   "metadata": {},
   "source": [
    "Local Outlier Factor illustrated\n",
    "Parameters include two primary settings: n_neighbors and contamination.\n",
    "\n",
    "Autoencoder Algorithm\n",
    "Autoencoders are unsupervised machine learning models based on neural networks. They comprise two main parts: a) encoder and b) decoder. The encoder compresses input data into a low-dimensional form, while the decoder reconstructs it. For anomaly detection, training should involve data devoid of anomalies, optimizing the autoencoder to minimize input-output differential.\n",
    "\n",
    "In anomaly contexts, training models on test data allows identifying anomalies via high reconstruction errors.\n",
    "\n",
    "Here’s an autoencoder model code generated by ChatGPT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78c67d-75f0-4a7f-bd23-befb838192a5",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "def detect_anomalies_with_autoencoder(series, window_size=20, latent_dim=3, epochs=100):\n",
    "    # Prepare the input data\n",
    "    X = []\n",
    "    for i in range(len(series) - window_size):\n",
    "        X.append(series[i:i+window_size])\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Define the autoencoder architecture\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(window_size,)),\n",
    "        keras.layers.Dense(latent_dim, activation='relu'),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(window_size, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, X, epochs=epochs, verbose=0)\n",
    "    \n",
    "    # Use the trained autoencoder to detect anomalies\n",
    "    X_pred = model.predict(X)\n",
    "    mse = np.mean(np.power(X - X_pred, 2), axis=1)\n",
    "    threshold = np.percentile(mse, 95)\n",
    "    anomalies = series.iloc[window_size:][mse >= threshold]\n",
    "    \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566910ff-ad5f-437a-bf25-27f80370a3fa",
   "metadata": {},
   "source": [
    "As mentioned earlier, “blind” application might make it seem like a near-“perfect” solution. 😂 Examine the code (access link at the conclusion).\n",
    "\n",
    "ARIMA example\n",
    "\n",
    "Notice that the overlap between training and test sets is deliberate to observe model behavior on historical data.\n",
    "\n",
    "However, using the ARIMA model properly warrants cautious preparation. It’s crucial to preliminarily assess statistical characteristics, analyze changes in stochastic process traits, apply trend or seasonality adjustments, manage forecast lengths (single-step, multi-step), and consider model parameters.\n",
    "\n",
    "We now have a preliminary grasp of leveraging ChatGPT for data analysis topic exploration. Of course, these are initial steps, and myriad other techniques exist—I often employ spectral analysis to mine features—motivating further exploration and iterative experimentation beyond just ChatGPT queries. Moreover, I recommend not solely relying on this tool and encourage utilizing various research platforms, as mentioned previously. Nonetheless, ChatGPT can significantly expedite processes.\n",
    "\n",
    "Summary and Conclusion\n",
    "This guide illustrates leveraging ChatGPT in exploring a specific data analysis topic.\n",
    "Utilized ChatGPT to understand anomaly detection in time series data.\n",
    "Examined examples covering statistical and machine learning approaches.\n",
    "Clarified differences between anomaly and change point detection.\n",
    "Finally, it’s crucial to treat ChatGPT as purely a tool—albeit a powerful one—requiring critical evaluation and thoughtful verification of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490057d-fa55-4e99-afeb-e0a84a8ba5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
